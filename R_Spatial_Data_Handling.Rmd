---
title: "R Spatial Data Handling"
output: html_notebook
---

This is an R notebook covering the functionality of the Spatial Data Handling Section of the GeoDa workbook. An overview of the the contents of the notebook:

-Loading Libraries
-Loading the data
-converting Dates from String to Date format
-Filtering the table for specific entries
-Renaming colunms
-Converting to a shapefile
-Checking and adding/adjusting projections
-Dealing with community area missing information
-Merging Data
-Bivariate Operations
-Chloropleth mapping

The libraries to be used:
-tidyverse: used to filter data and select columns

-lubridate: will be used to select information out of the date format when filtering the data

-sf: will be used to convert our data set into a simple feature object, to read in the boundary file,
     and perform point in polygon on the data set to fill in missing community area information
     
-dplry: will be used to perform table joins and create abandoned vehicle counts for each community area

-tmap: used to make the choropleth maps

-pdftools: will be used to read and parse a pdf for chicago community area population information

-RSocrata: is used to read the data directly from the chicago data portal  

Below are the libraries necessary for the R version of Spatial Data Handling 
```{r}
library(tidyverse)
library(lubridate)
library(sf)
library(tmap)
library(pdftools)
library(RSocrata)
```
Setting the working directory
```{r}
setwd("~/Downloads/lab_tutorials/Spatial Data Handling")
```

Data is from the Chicago data portal and is 311 calls about abandoned vehicles

Loading the data from the working directry
```{r}
stem <- "https://data.cityofchicago.org/resource"
file1 <- "/suj7-cg3j.csv"

vehicle_data <- read.socrata(paste0(stem,file1))
head(vehicle_data)
```

After looking at the first few rows of the data, most of the information is in String format. In order to select rows by date, we first need to convert the date from a String to a date format using the as.Date function.

```{r}
vehicle_data$credate <- as.Date(vehicle_data$creation_date,"%Y-%m-%d")
head(vehicle_data)
```

Now that the Dates have been converted from strings to dates, we can select different time periods using the filter function and the date format. We can use the year() and month() functions from lubridate in our filration process to get observations in the year 2016 and month of september.

```{r}
vehicle_data <- vehicle_data %>% filter(year(vehicle_data$credate) == 2016)
vehicle_data <- vehicle_data %>% filter(month(vehicle_data$credate) == 9)
head(vehicle_data)
```

We can select columns using the tidyverse select command. We then rename the columns for later convenience, as the column names are kind of long and contain the (`) symbol

```{r}
names(vehicle_data)

#selects columns
selected_data <- vehicle_data %>% select(x_coordinate, y_coordinate, longitude, latitude, credate, street_address, zip_code, ward, police_district, community_area)

#view names of columns
names(selected_data)





```


Next is turning the selected data into a shapefile, but we will have to filter out the point that doesnt have a latitude and longitude value or dont have an x and y coordinate.


```{r}

#filtering
selected_data <- selected_data %>% filter(!(is.na(latitude)))

coord_data <- selected_data %>% filter(!(is.na(x_coordinate)))


#conerting to a simple features object
sf_data = st_as_sf(coord_data, coords = c("longitude", "latitude"), agr = "constant")

#checking ours point plot... won't contain useful information
plot(sf_data)
```

After the data frame is converted to a spatial points data frame, we need to check the projection and add one if the information is absent

```{r}

#checks the projection of the point layer
st_crs(sf_data)
#returns NA, so we need to set a projection

#The projection information is missing, so we will add the one for chicago: +init=epsg:32616
sf_data <- sf_data %>% st_set_crs(4326)

sf_data <- sf_data %>% st_transform(32616)
#check the projection again and we should have EPSG: 32616
st_crs(sf_data)
write_csv(coord_data, "september_16_vehicles.csv")

```






We will next read the shapefile into our R environment using SF functionality. The function requires a correct file path if the file is not directly in your working directory.

```{r}
chicago_bound <- st_read("boundaries/boundary.shp", agr = "constant")

plot(chicago_bound)
```









Here we check the projection of the boundary file. It has one, but we need to it to be the same as the point data, so we use st_transform to change the projection

```{r}
#projection of shapefile

#checks the projection of the point layer
st_crs(chicago_bound)

#returns EPSG: 4326, so we need to transform to 32616

chicago_bound <- chicago_bound %>% st_transform(32616)

#check the projection again and we should have EPSG: 32616
st_crs(sf_data)

```






Now we need to deal with the missing community area information in the data frame. Rather than removing those observations from the data we are going to use point in polygon fill in the missing values, using their x and y values and the chicago_boundary polygons



```{r}
#check projections
st_crs(chicago_bound)
st_crs(sf_data)

#join data to get all comm area data
full_comm_data <- st_join(sf_data, chicago_bound["area_num_1"])

```



Now that we have the missing community area information, we can move forward and get counts of abandoned vechicles for each communtiy area. To do this we will use the dplry function count(). This function will give us a dataframe with the counts of a specified variable. In this case it is area_num_1. Next we will use the select column to drop the added geometry section of our resulting data frame to make joining the data more convenient. Lastly, in this code chunk we rename the count column "n" to a more descriptive name 
```{r}

full_comm_data$area_num_1 <- as.numeric(full_comm_data$area_num_1)

#creating counts of abandoned vehicles per community area
counts <- full_comm_data %>% count(area_num_1) 
counts <- select(as.data.frame(counts), -geometry)

#giving the count column a more descriptive name for mapping purposes
names(counts)[2] <- "abandoned_vehicle_counts"
```



Here we will join the counts data to chicago_bound, so we can make the first map. First we change the chicago_bound area_num_1 column to numeric to keep the the join data of the same type. We then do a left join of chicago_bound and the counts data frame by the communtiy area id column, which is call "area_num_1"
```{r}
chicago_bound$area_num_1 <- as.numeric(chicago_bound$area_num_1)
chicago_bound_counts <- left_join(chicago_bound, counts, by = "area_num_1")
```





Now we have enough to make the first choropleth map. We use the tmap library to create a basic map of the counts. Note this information is not particularly useful because these counts will be heavily dependent on the population of a communtiy area. We have to get population information about the community areas to be able to sepate this effect.
```{r}
tm_shape(chicago_bound_counts) +
  tm_polygons("abandoned_vehicle_counts", id = "area_num_1")
```




Now we need population information about each of the community areas to get an accurate depiction of the spatial varation of abandon vehicles in Chicago. A choropleth map with the just the counts of abandoned vehicles in an area does not say much because a factor like this is likely correlated with population size. We need to examine the variations in a rate form to separate these effects to make any conclusions about the data. The information we need is in the form of a pdf, which is not particularly convenient, but is still a far better option than hard typing in the population information for each community area. We first start by reading the pdf into the R environment using the pdf_text() function 
```{r}
#reading the pdf
dat <-pdf_text("Census_2010_and_2000_CA_Populations.pdf")
```







The result data structure is just one large string and we will have to take advatage of the layout of the table get the information.
```{r}
length(dat[[1]])
```





We can carry this out one step at a time, but in order to reach some level of abstraction, we turn it into a loop. First, we initialize the neighborhood list (nnlist) with an empty character [first line below]. Next comes the loop for values of the index igoing from 1 to 2 (recall that the list has only two elements, one for each page). Since each element is just one long string, we use the strsplitstring split command to separate the long string into a list of one string for each line,by using the return character \nas the separator [line 3 in the code snippet below]. We then extract the first element of the resulting list using the double bracket notation (this is a side effect of the way lists are organized --if this seems strange, check the R intro document). We subsequently strip the first four lines from this list (these lines do not contain data --of course the only way we know this is by carefully checking the structure of the pdf file).To streamline the resulting data structure (again, a special characteristic of lists) we turn it into a simple vector by means of unlist. This then allows us to concatenate the result to the current nnlist(initially, just an empty character, after the first step it contains the empty character and the first page, and at the end it has the empty character, the first and the second page).
```{r}
nnlist <-""
for (i in 1:2) {
  ppage <-strsplit(dat[[i]],split="\n")
  nni <-ppage[[1]]
  nni <-nni[-(1:4)]
  nnu <-unlist(nni)
  nnlist <-c(nnlist,nnu)
  }
length(nnlist)
```



The resulting list has 79 elements. Now, we still need to strip the first (empty) element, and the last element, which is nothing but the totals. We thus extract the elements from 2to length -1.


```{r}
nnlist <-nnlist[2:(length(nnlist)-1)]
length(nnlist)
```



We first initialize a vector of zeros to hold the population values. It is the preferred approach to initialize a vector first if one knows its size, rather than having it grow by appending rows or columns. We use the vectorcommand and specify the modeto numericand give the lengthas the length of the list.


```{r}
nnpop <-vector(mode="numeric",length=length(nnlist))
```


We again will use a loop to process each element of the list (each line of the table) one by one. We use the substrcommand to extract the characters between position 27 and 39 (these values were determined after taking a careful look at the structure of the table). However, there is still a problem, since the population values contain commas. We now do two things in one line of code. First, we use gsubto substitute the character ,by an empty"". We turn the result into a numeric value by means of as.numeric. We then assign this number to position iof the vector. The resulting vector nnpopcontains the population for each of the community areas.


```{r}
for (i in (1:length(nnlist))) {
  popchar <-substr(nnlist[i],start=27,stop=39)
  popval <-as.numeric(gsub(",","",popchar))
  nnpop[i] <-popval
  }
```



In addition to the vector of the population values, we also need a vector of ID values. Since the community area indicators are simple sequence numbers, we create such a vector to serve as the ID.


```{r}
nnid <-(1:length(nnlist))
```

We turn the vectors nnidand nnpopinto a data frame using the data.framecommand. Since the variable names assigned automatically are not that informative, we force them to NIDand POP2010using the namescommand. Also, as we did before, we make sure the ID variable is an integer (formerging in GeoDa) by means of as.integer( )

```{r}
neighpop <-data.frame(as.integer(nnid),nnpop)
head(neighpop)
```




Now that we have a data frame with the population data for each community area, we can proceed by joining the data to chicago_bound_counts. We will join do the join with the dplyr left_join() function in the same manner as used earlier for the counts. To do this, we will name the community area id section as area_num_1 and will join the dataframe to the sf object by this common variable.

```{r}
names(neighpop) <- c("area_num_1", "population")
chicago_bound_final <- left_join(chicago_bound_counts, neighpop, by = "area_num_1")
```




Now we will create a new column with abandoned vehicles per 1000 people using bivariate operations, the population data column, and the abandoned vehicle count column
```{r}
chicago_bound_final <- chicago_bound_final %>% mutate(vehicles_per_1000_people = abandoned_vehicle_counts / nnpop * 1000) 
```





Now that we have abandoned vehicles in terms of a rate, we can make a more meaningful map with the tmap library.

```{r}
tm_shape(chicago_bound_final) +
  tm_polygons("vehicles_per_1000_people", id = "area_num_1")
```







